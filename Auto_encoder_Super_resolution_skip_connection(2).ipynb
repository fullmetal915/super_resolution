{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sC1Haz99g8gf"
   },
   "source": [
    "Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-06-12T11:43:39.535381Z",
     "iopub.status.busy": "2023-06-12T11:43:39.534960Z",
     "iopub.status.idle": "2023-06-12T11:43:39.543736Z",
     "shell.execute_reply": "2023-06-12T11:43:39.542607Z",
     "shell.execute_reply.started": "2023-06-12T11:43:39.535348Z"
    },
    "id": "6-a8prdKg8gm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.ndimage import zoom\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import Reshape\n",
    "from keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import math\n",
    "import tifffile\n",
    "import os\n",
    "from pickle import load\n",
    "import numpy as np\n",
    "import imageio\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.layers import Conv2DTranspose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\otiwa\n"
     ]
    }
   ],
   "source": [
    "cd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3xefmPxg8gp"
   },
   "source": [
    "Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Kf3SgRm3g8gq"
   },
   "outputs": [],
   "source": [
    "#function to slice the image\n",
    "def slicing(lr_train_images, subset_size):\n",
    "    '''a simple but very effective function for slicing the input in to (128 X 128) and the ground truth into (256 X 256)'''\n",
    "\n",
    "    # Calculate the number of subsets\n",
    "    num_subsets = np.array(lr_train_images.shape[:2]) // subset_size\n",
    "\n",
    "    # Initialize an empty list to store the subsets\n",
    "    subsets = []\n",
    "\n",
    "    # Loop over the number of subsets in rows and columns\n",
    "    for i in range(num_subsets[0]):\n",
    "        for j in range(num_subsets[1]):\n",
    "            # Calculate the starting indices of the subset\n",
    "            start_row = i * subset_size\n",
    "            start_col = j * subset_size\n",
    "\n",
    "            # Extract the subset from the image\n",
    "            subset = lr_train_images[start_row:start_row+subset_size, start_col:start_col+subset_size]\n",
    "\n",
    "            # Append the subset to the list\n",
    "            subsets.append(subset)\n",
    "\n",
    "    # Convert the list of subsets to a numpy array\n",
    "    subsets_array = np.array(subsets)\n",
    "\n",
    "    return subsets_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2BfSzI7jg8gr"
   },
   "outputs": [],
   "source": [
    "#function to preprocess\n",
    "def preprocess_image(image, no, count):\n",
    "    '''The preprocessing step involves normalisation between -1 to 1 and slicing the image'''\n",
    "    \n",
    "    count += 1\n",
    "    image_np = np.array(image)\n",
    "    print(f'the image {count} min: {np.min(image)}')\n",
    "    print(f'the image {count} max: {np.max(image)}')\n",
    "    print(f'the image {count} mean: {np.mean(image)}')\n",
    "    # print(f'the image {count} mean: {np.min(image)-np.mean(image)}')\n",
    "    print(f'the dimension of {count} image: {image_np.shape}')\n",
    "    min_value = np.min(image_np)\n",
    "    max_value = np.max(image_np)\n",
    "    image_np = (image_np - min_value) * (2.0) / (max_value - min_value) - 1.0\n",
    "\n",
    "    # Perform slicing on the preprocessed array\n",
    "    image_np_x_slice = slicing(image_np, no)\n",
    "    image_np_x_slice1 = None\n",
    "\n",
    "    if no == 128:\n",
    "        # Set the desired scale factor for upsampling\n",
    "        scale_factor = 2\n",
    "        # Upsample the image using zoom\n",
    "        image_np_x_slice1 = zoom(image_np_x_slice, (1, scale_factor, scale_factor, 1), order=3)\n",
    "        image_np_x_slice1.shape\n",
    "    else:\n",
    "        image_np_x_slice1 = image_np_x_slice\n",
    "\n",
    "    return image_np_x_slice1, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aM9Mj--ljoJO"
   },
   "outputs": [],
   "source": [
    "def data_augmentation(images):\n",
    "    '''data augmentaion is done by rotation and flipping'''\n",
    "\n",
    "    #Create empty arrays to store rotation and flipped images\n",
    "    rotated_90 = np.empty_like(images)\n",
    "    rotated_180 = np.empty_like(images)\n",
    "    rotated_270 = np.empty_like(images)\n",
    "    flipped_horizontal = np.empty_like(images)\n",
    "    flipped_vertical = np.empty_like(images)\n",
    "\n",
    "    # Rotate each image in the ndarray\n",
    "    for i in range(images.shape[0]):\n",
    "        for j in range(images.shape[3]):\n",
    "            rotated_90[i, :, :, j] = np.rot90(images[i, :, :, j], k=1)\n",
    "            rotated_180[i, :, :, j] = np.rot90(images[i, :, :, j], k=2)\n",
    "            rotated_270[i, :, :, j] = np.rot90(images[i, :, :, j], k=3)\n",
    "            flipped_horizontal[i, :, :, j] = np.flip(images[i, :, :, j], axis=1)\n",
    "            flipped_vertical[i, :, :, j] = np.flip(images[i, :, :, j], axis=0)\n",
    "\n",
    "\n",
    "    # Concatenate the original, rotated, flipped horizontally, and flipped vertically images\n",
    "    final_images = np.concatenate((images, rotated_90, rotated_180, rotated_270, flipped_horizontal, flipped_vertical), axis=0)\n",
    "\n",
    "    return final_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0rlKYOVhg8gu",
    "outputId": "59e486b7-799f-4e9a-b85d-d1942cb18765"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01jan19_merged_sample2_ls.tif', '01jan19_merged_sample1_ls.tif']\n",
      "the image 1 min: 2.0\n",
      "the image 1 max: 1311.0\n",
      "the image 1 mean: 362.4297180175781\n",
      "the dimension of 1 image: (567, 572, 4)\n",
      "the image 2 min: 1.0\n",
      "the image 2 max: 1337.0\n",
      "the image 2 mean: 373.217041015625\n",
      "the dimension of 2 image: (295, 569, 4)\n",
      "the ouput dimension for image 0: (16, 256, 256, 4)\n",
      "the ouput dimension for image 1: (8, 256, 256, 4)\n",
      "shape of training example: (144, 256, 256, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\otiwa\\AppData\\Local\\Temp\\ipykernel_19252\\3980498972.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  preprocessed_array = np.array(preprocessed_images)\n"
     ]
    }
   ],
   "source": [
    "#training examples\n",
    "# Folder path containing the images\n",
    "folder_path = 'final_year_project/auto-encoder/dataset/ls'\n",
    "\n",
    "# List all files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "file_list[0], file_list[1] = file_list[1], file_list[0]\n",
    "print(file_list)\n",
    "# Iterate over the files and preprocess the images\n",
    "count = 0\n",
    "preprocessed_images = []\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        image = imageio.imread(file_path)\n",
    "        preprocessed_image, count = preprocess_image(image, 128, count)\n",
    "        preprocessed_images.append(preprocessed_image)\n",
    "\n",
    "\n",
    "# Convert the list of preprocessed images to a NumPy array\n",
    "preprocessed_array = np.array(preprocessed_images)\n",
    "preprocessed_array = preprocessed_array.reshape(preprocessed_array.shape[0], -1)\n",
    "\n",
    "for i, j in enumerate(range(preprocessed_array.shape[0])):\n",
    "    print(f\"the ouput dimension for image {i}: {preprocessed_array[j][0].shape}\")\n",
    "    count = i\n",
    "\n",
    "training_sample = np.concatenate((preprocessed_array[0][0], preprocessed_array[1][0]), axis=0)\n",
    "\n",
    "#data augmentation\n",
    "training_sample = data_augmentation(training_sample)\n",
    "print(f'shape of training example: {training_sample.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZawvWpC_ySi2",
    "outputId": "e5906f55-b573-44f0-f68c-a3e5a7ade6e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0043683"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sample.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VtcYJqOhg8gw",
    "outputId": "1acc6b53-a424-4d36-d0bb-c538bd5af587"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['15jan19_merged_sample1_hs.tif', '15may19_merged_sample2_hs.tif']\n",
      "the image 1 min: 1.0\n",
      "the image 1 max: 266.0\n",
      "the image 1 mean: 95.2601547241211\n",
      "the dimension of 1 image: (589, 1138, 4)\n",
      "the image 2 min: 24.0\n",
      "the image 2 max: 598.0\n",
      "the image 2 mean: 152.09535217285156\n",
      "the dimension of 2 image: (1134, 1144, 4)\n",
      "the ouput dimension for image 0: (8, 256, 256, 4)\n",
      "the ouput dimension for image 1: (16, 256, 256, 4)\n",
      "shape of training example: (144, 256, 256, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\otiwa\\AppData\\Local\\Temp\\ipykernel_19252\\2535726390.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  preprocessed_array = np.array(preprocessed_images)\n"
     ]
    }
   ],
   "source": [
    "#testing examples\n",
    "# Folder path containing the images\n",
    "folder_path = 'final_year_project/auto-encoder/dataset/hs'\n",
    "\n",
    "# List all files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "print(file_list)\n",
    "# Iterate over the files and preprocess the images\n",
    "count = 0\n",
    "preprocessed_images = []\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        image = imageio.imread(file_path)\n",
    "        preprocessed_image, count = preprocess_image(image, 256, count)\n",
    "        preprocessed_images.append(preprocessed_image)\n",
    "\n",
    "\n",
    "# Convert the list of preprocessed images to a NumPy array\n",
    "preprocessed_array = np.array(preprocessed_images)\n",
    "preprocessed_array = preprocessed_array.reshape(preprocessed_array.shape[0], -1)\n",
    "\n",
    "for i, j in enumerate(range(preprocessed_array.shape[0])):\n",
    "    print(f\"the ouput dimension for image {i}: {preprocessed_array[j][0].shape}\")\n",
    "    count = i\n",
    "\n",
    "testing_sample = np.concatenate((preprocessed_array[0][0], preprocessed_array[1][0]), axis=0)\n",
    "\n",
    "#data_augmentation\n",
    "testing_sample = data_augmentation(testing_sample)\n",
    "print(f'shape of training example: {testing_sample.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ynjGFw1eplzS",
    "outputId": "d8bea574-e204-4cbe-b1b7-ca55f26f0bd1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_sample.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2Q2tTNHg8gz"
   },
   "source": [
    "antoencoder architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qRJqktygg8gz"
   },
   "outputs": [],
   "source": [
    "def auto_encoder(model, img_h=256, img_w=256, learning_rate=0.001):\n",
    "    '''the auto encoder architecture consist of Encoder -> Latent space -> Decoder'''\n",
    "\n",
    "    #encoder model\n",
    "    input1 = Input(shape=(img_h, img_w, 4))\n",
    "    encoder_output1 = Conv2D(16, (3, 3), activation='relu', padding='same', strides=2)(input1)\n",
    "    encoder_output2 = Conv2D(32, (3, 3), activation='relu', padding='same', strides=2)(encoder_output1)\n",
    "    encoder_output3 = Conv2D(32, (3, 3), activation='relu', padding='same', strides=2)(encoder_output2)\n",
    "    encoder_output4 = Conv2D(32, (3, 3), activation='relu', padding='same', strides=2)(encoder_output3)\n",
    "    encoder_output5 = Conv2D(32, (3, 3), activation='relu', padding='same', strides=2)(encoder_output4)\n",
    "    encoder_output6 = Conv2D(32, (3, 3), activation='relu', padding='same', strides=2)(encoder_output5)\n",
    "\n",
    "    #latent space\n",
    "    decoder_output = Conv2D(128, (3, 3), activation='relu', padding='same', strides=2)(encoder_output6)\n",
    "\n",
    "    #decoder model\n",
    "    decoder_output1 = Conv2DTranspose(32, (3, 3), activation='relu', padding='same', strides=2)(decoder_output)\n",
    "    decoder_output1 = Conv2DTranspose(32, (3, 3), activation='relu', padding='same', strides=2)(decoder_output1)\n",
    "    decoder_output1 = BatchNormalization()(decoder_output1)\n",
    "    decoder_output1_skip = Add()([encoder_output5, decoder_output1])\n",
    "\n",
    "    decoder_output1 = Conv2DTranspose(64, (3, 3), activation='relu', padding='same', strides=2)(decoder_output1_skip)\n",
    "    decoder_output1 = Conv2DTranspose(64, (3, 3), activation='relu', padding='same', strides=2)(decoder_output1)\n",
    "    decoder_output2 = Conv2DTranspose(32, (3, 3), activation='relu', padding='same', strides=2)(decoder_output1)\n",
    "    decoder_output2 = BatchNormalization()(decoder_output2)\n",
    "    decoder_output2_skip = Add()([encoder_output2, decoder_output2])\n",
    "\n",
    "    decoder_output3 = Conv2DTranspose(16, (3, 3), activation='relu', padding='same', strides=2)(decoder_output2_skip)\n",
    "    decoder_output3 = BatchNormalization()(decoder_output3)\n",
    "    decoder_output3_skip = Add()([encoder_output1, decoder_output3])\n",
    "\n",
    "    decoder_output4 = Conv2DTranspose(4, (3, 3), activation='tanh', padding='same', strides=2)(decoder_output3_skip)\n",
    "    decoder_output4 = BatchNormalization()(decoder_output4)\n",
    "    decoder_output4_skip = Add()([input1, decoder_output4])\n",
    "    # decoder_output5 = Conv2DTranspose(4, (3, 3), activation='relu', padding='same', strides=2)(decoder_output4_skip)\n",
    "\n",
    "    # model definition\n",
    "    model = Model(inputs=input1, outputs=decoder_output4_skip)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['acc'])\n",
    "\n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RgDtNUSzg8g0",
    "outputId": "6ee57a42-95f5-4aba-f7c4-c582bd7eb6f4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 256, 256, 4  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 128, 128, 16  592         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 64, 64, 32)   4640        ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 32)   9248        ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 16, 16, 32)   9248        ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 8, 8, 32)     9248        ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 4, 4, 32)     9248        ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 2, 2, 128)    36992       ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 4, 4, 32)    36896       ['conv2d_6[0][0]']               \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 8, 8, 32)    9248        ['conv2d_transpose[0][0]']       \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 8, 8, 32)    128         ['conv2d_transpose_1[0][0]']     \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 8, 8, 32)     0           ['conv2d_4[0][0]',               \n",
      "                                                                  'batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 16, 16, 64)  18496       ['add[0][0]']                    \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2DTran  (None, 32, 32, 64)  36928       ['conv2d_transpose_2[0][0]']     \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_transpose_4 (Conv2DTran  (None, 64, 64, 32)  18464       ['conv2d_transpose_3[0][0]']     \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 64, 64, 32)  128         ['conv2d_transpose_4[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 64, 64, 32)   0           ['conv2d_1[0][0]',               \n",
      "                                                                  'batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose_5 (Conv2DTran  (None, 128, 128, 16  4624       ['add_1[0][0]']                  \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 128, 128, 16  64         ['conv2d_transpose_5[0][0]']     \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 128, 128, 16  0           ['conv2d[0][0]',                 \n",
      "                                )                                 'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose_6 (Conv2DTran  (None, 256, 256, 4)  580        ['add_2[0][0]']                  \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 256, 256, 4)  16         ['conv2d_transpose_6[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 256, 256, 4)  0           ['input_1[0][0]',                \n",
      "                                                                  'batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 204,788\n",
      "Trainable params: 204,620\n",
      "Non-trainable params: 168\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create the model outside the TensorFlow function\n",
    "model = auto_encoder(model= None, img_h=256, img_w=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORBJE8JDg8g1"
   },
   "source": [
    "training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kGUlCcEZg8g1",
    "outputId": "c23ca5cb-a606-46fd-e88a-5e8b1678444a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "144/144 [==============================] - 9s 38ms/step - loss: 0.4392 - acc: 0.6562\n",
      "Epoch 2/50\n",
      "144/144 [==============================] - 5s 36ms/step - loss: 0.0404 - acc: 0.8667\n",
      "Epoch 3/50\n",
      "144/144 [==============================] - 6s 39ms/step - loss: 0.0362 - acc: 0.8747\n",
      "Epoch 4/50\n",
      "144/144 [==============================] - 6s 40ms/step - loss: 0.0349 - acc: 0.8762\n",
      "Epoch 5/50\n",
      "144/144 [==============================] - 6s 40ms/step - loss: 0.0343 - acc: 0.8769\n",
      "Epoch 6/50\n",
      "144/144 [==============================] - 5s 38ms/step - loss: 0.0341 - acc: 0.8771\n",
      "Epoch 7/50\n",
      "144/144 [==============================] - 6s 40ms/step - loss: 0.0339 - acc: 0.8773\n",
      "Epoch 8/50\n",
      "144/144 [==============================] - 5s 37ms/step - loss: 0.0338 - acc: 0.8775\n",
      "Epoch 9/50\n",
      "144/144 [==============================] - 6s 39ms/step - loss: 0.0338 - acc: 0.8775\n",
      "Epoch 10/50\n",
      "144/144 [==============================] - 5s 38ms/step - loss: 0.0337 - acc: 0.8776\n",
      "Epoch 11/50\n",
      "144/144 [==============================] - 5s 37ms/step - loss: 0.0336 - acc: 0.8776\n",
      "Epoch 12/50\n",
      "144/144 [==============================] - 5s 37ms/step - loss: 0.0336 - acc: 0.8776\n",
      "Epoch 13/50\n",
      "144/144 [==============================] - 5s 37ms/step - loss: 0.0336 - acc: 0.8777\n",
      "Epoch 14/50\n",
      "144/144 [==============================] - 6s 40ms/step - loss: 0.0335 - acc: 0.8776\n",
      "Epoch 15/50\n",
      "144/144 [==============================] - 6s 39ms/step - loss: 0.0335 - acc: 0.8776\n",
      "Epoch 16/50\n",
      "144/144 [==============================] - 6s 39ms/step - loss: 0.0335 - acc: 0.8777\n",
      "Epoch 17/50\n",
      "144/144 [==============================] - 6s 40ms/step - loss: 0.0335 - acc: 0.8777\n",
      "Epoch 18/50\n",
      "144/144 [==============================] - 6s 43ms/step - loss: 0.0335 - acc: 0.8777\n",
      "Epoch 19/50\n",
      "144/144 [==============================] - 5s 37ms/step - loss: 0.0335 - acc: 0.8776\n",
      "Epoch 20/50\n",
      "144/144 [==============================] - 6s 40ms/step - loss: 0.0334 - acc: 0.8776\n",
      "Epoch 21/50\n",
      "144/144 [==============================] - 6s 43ms/step - loss: 0.0334 - acc: 0.8777\n",
      "Epoch 22/50\n",
      "144/144 [==============================] - 6s 44ms/step - loss: 0.0334 - acc: 0.8776\n",
      "Epoch 23/50\n",
      "144/144 [==============================] - 6s 44ms/step - loss: 0.0334 - acc: 0.8777\n",
      "Epoch 24/50\n",
      "144/144 [==============================] - 6s 38ms/step - loss: 0.0334 - acc: 0.8776\n",
      "Epoch 25/50\n",
      "144/144 [==============================] - 5s 37ms/step - loss: 0.0334 - acc: 0.8776\n",
      "Epoch 26/50\n",
      "144/144 [==============================] - 5s 38ms/step - loss: 0.0334 - acc: 0.8776\n",
      "Epoch 27/50\n",
      "144/144 [==============================] - 6s 40ms/step - loss: 0.0334 - acc: 0.8776\n",
      "Epoch 28/50\n",
      "144/144 [==============================] - 6s 42ms/step - loss: 0.0333 - acc: 0.8776\n",
      "Epoch 29/50\n",
      "144/144 [==============================] - 6s 42ms/step - loss: 0.0334 - acc: 0.8776\n",
      "Epoch 30/50\n",
      "144/144 [==============================] - 5s 32ms/step - loss: 0.0334 - acc: 0.8776\n",
      "Epoch 31/50\n",
      "144/144 [==============================] - 6s 39ms/step - loss: 0.0334 - acc: 0.8776\n",
      "Epoch 32/50\n",
      "144/144 [==============================] - 6s 39ms/step - loss: 0.0334 - acc: 0.8776\n",
      "Epoch 33/50\n",
      "144/144 [==============================] - 6s 41ms/step - loss: 0.0334 - acc: 0.8776\n",
      "Epoch 34/50\n",
      "144/144 [==============================] - 6s 41ms/step - loss: 0.0334 - acc: 0.8776\n",
      "Epoch 35/50\n",
      "144/144 [==============================] - 6s 41ms/step - loss: 0.0334 - acc: 0.8776\n",
      "Epoch 36/50\n",
      "144/144 [==============================] - 6s 41ms/step - loss: 0.0334 - acc: 0.8776\n",
      "Epoch 37/50\n",
      "144/144 [==============================] - 6s 40ms/step - loss: 0.0333 - acc: 0.8776\n",
      "Epoch 38/50\n",
      "144/144 [==============================] - 6s 38ms/step - loss: 0.0334 - acc: 0.8776\n",
      "Epoch 39/50\n",
      "144/144 [==============================] - 5s 36ms/step - loss: 0.0333 - acc: 0.8776\n",
      "Epoch 40/50\n",
      "144/144 [==============================] - 6s 40ms/step - loss: 0.0333 - acc: 0.8776\n",
      "Epoch 41/50\n",
      "144/144 [==============================] - 6s 41ms/step - loss: 0.0333 - acc: 0.8776\n",
      "Epoch 42/50\n",
      "144/144 [==============================] - 6s 42ms/step - loss: 0.0333 - acc: 0.8776\n",
      "Epoch 43/50\n",
      "144/144 [==============================] - 6s 40ms/step - loss: 0.0334 - acc: 0.8776\n",
      "Epoch 44/50\n",
      "144/144 [==============================] - 5s 38ms/step - loss: 0.0333 - acc: 0.8776\n",
      "Epoch 45/50\n",
      "144/144 [==============================] - 6s 39ms/step - loss: 0.0333 - acc: 0.8776\n",
      "Epoch 46/50\n",
      "144/144 [==============================] - 6s 39ms/step - loss: 0.0333 - acc: 0.8776\n",
      "Epoch 47/50\n",
      "144/144 [==============================] - 6s 42ms/step - loss: 0.0333 - acc: 0.8776\n",
      "Epoch 48/50\n",
      "144/144 [==============================] - 6s 40ms/step - loss: 0.0333 - acc: 0.8776\n",
      "Epoch 49/50\n",
      "144/144 [==============================] - 6s 41ms/step - loss: 0.0334 - acc: 0.8776\n",
      "Epoch 50/50\n",
      "144/144 [==============================] - 6s 41ms/step - loss: 0.0334 - acc: 0.8776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2838a6d2160>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(training_sample, testing_sample, batch_size=1, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "PHmWz-E2g8g2"
   },
   "outputs": [],
   "source": [
    "# Save the model weights to the current directory\n",
    "model.save_weights('autoencoder_weights_new.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LSFRrs3MNO9f",
    "outputId": "3c239068-dd67-4756-f63a-ffcfce826d71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sample[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1fZJyfpg8g2"
   },
   "source": [
    "predicting the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y1FIxxQjg8g2",
    "outputId": "a8833811-66b6-4b01-b1d2-7fa79c3d11be",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 256, 256, 4  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 128, 128, 16  592         ['input_2[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 64, 64, 32)   4640        ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 32, 32, 32)   9248        ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 16, 16, 32)   9248        ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 8, 8, 32)     9248        ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 4, 4, 32)     9248        ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 2, 2, 128)    36992       ['conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose_7 (Conv2DTran  (None, 4, 4, 32)    36896       ['conv2d_13[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_transpose_8 (Conv2DTran  (None, 8, 8, 32)    9248        ['conv2d_transpose_7[0][0]']     \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 8, 8, 32)    128         ['conv2d_transpose_8[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 8, 8, 32)     0           ['conv2d_11[0][0]',              \n",
      "                                                                  'batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose_9 (Conv2DTran  (None, 16, 16, 64)  18496       ['add_4[0][0]']                  \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_transpose_10 (Conv2DTra  (None, 32, 32, 64)  36928       ['conv2d_transpose_9[0][0]']     \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " conv2d_transpose_11 (Conv2DTra  (None, 64, 64, 32)  18464       ['conv2d_transpose_10[0][0]']    \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 64, 64, 32)  128         ['conv2d_transpose_11[0][0]']    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 64, 64, 32)   0           ['conv2d_8[0][0]',               \n",
      "                                                                  'batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose_12 (Conv2DTra  (None, 128, 128, 16  4624       ['add_5[0][0]']                  \n",
      " nspose)                        )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 128, 128, 16  64         ['conv2d_transpose_12[0][0]']    \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 128, 128, 16  0           ['conv2d_7[0][0]',               \n",
      "                                )                                 'batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_transpose_13 (Conv2DTra  (None, 256, 256, 4)  580        ['add_6[0][0]']                  \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 256, 256, 4)  16         ['conv2d_transpose_13[0][0]']    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 256, 256, 4)  0           ['input_2[0][0]',                \n",
      "                                                                  'batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 204,788\n",
      "Trainable params: 204,620\n",
      "Non-trainable params: 168\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "5/5 [==============================] - 1s 117ms/step\n"
     ]
    }
   ],
   "source": [
    "# Rebuild the autoencoder model\n",
    "model = auto_encoder(model=None, img_h=256, img_w=256)\n",
    "\n",
    "# Load the saved weights\n",
    "model.load_weights('autoencoder_weights_new.h5')\n",
    "\n",
    "# Generate predictions\n",
    "predictions = model.predict(training_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u61w6fsKg8g4"
   },
   "source": [
    "saving the ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NYm_WTctg8g4",
    "outputId": "f0357175-a6bb-464b-f105-6cf2d40b947f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download the predicted output: [predicted_output.tif](final_year_project\\auto-encoder1\\dataset\\predicted[143].tif)\n"
     ]
    }
   ],
   "source": [
    "# Convert the data type of the predictions array to a suitable format\n",
    "# Modify this based on the requirements of your predicted output\n",
    "# predictions = predictions.squeeze()  # Remove singleton dimensions if present\n",
    "# predictions = np.uint8(predictions * 255)  # Scale values to 0-255 range and convert to uint8\n",
    "\n",
    "# Create a directory to save the predictions if it doesn't exist\n",
    "directory = r'final_year_project\\auto-encoder1\\dataset'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for i in range(predictions.shape[0]):\n",
    "    # Save the predicted output as a TIFF file\n",
    "    output_path = os.path.join(directory, f'predicted{[i]}.tif')\n",
    "    tifffile.imwrite(output_path, predictions[i], planarconfig='contig')\n",
    "\n",
    "# Display a download link\n",
    "print(f\"Download the predicted output: [predicted_output.tif]({output_path})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6OimgcpBXLWD",
    "outputId": "1d9a9f6e-d2ac-4831-8ec3-d13ecb6a98b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download the predicted output: [predicted_output.tif](final_year_project\\auto-encoder3\\dataset\\real_input[143].tif)\n"
     ]
    }
   ],
   "source": [
    "# Create a directory to save the predictions if it doesn't exist\n",
    "directory = r'final_year_project\\auto-encoder3\\dataset'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for i in range(predictions.shape[0]):\n",
    "    # Save the predicted output as a TIFF file\n",
    "    output_path = os.path.join(directory, f'real_input{[i]}.tif')\n",
    "    tifffile.imwrite(output_path, training_sample[i], planarconfig='contig')\n",
    "\n",
    "# Display a download link\n",
    "print(f\"Download the predicted output: [predicted_output.tif]({output_path})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJJF9YIcfDGV",
    "outputId": "9655104a-ba76-4166-a8fc-6623c6eb5aac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download the predicted output: [predicted_output.tif](final_year_project\\auto-encoder2\\dataset\\real_output[143].tif)\n"
     ]
    }
   ],
   "source": [
    "# Create a directory to save the predictions if it doesn't exist\n",
    "directory = r'final_year_project\\auto-encoder2\\dataset'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for i in range(predictions.shape[0]):\n",
    "    # Save the predicted output as a TIFF file\n",
    "    output_path = os.path.join(directory, f'real_output{[i]}.tif')\n",
    "    tifffile.imwrite(output_path, testing_sample[i], planarconfig='contig')\n",
    "\n",
    "# Display a download link\n",
    "print(f\"Download the predicted output: [predicted_output.tif]({output_path})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QWWgazbjn79V"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(r'final_year_project\\auto-encoder2\\dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0RpN1WWg8g5"
   },
   "source": [
    "Peak signal to noise ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Yj58Aqjmg8g6"
   },
   "outputs": [],
   "source": [
    "def calculate_psnr(original, predicted):\n",
    "    mse = np.mean((original - predicted) ** 2)\n",
    "    max_value = np.max(original)\n",
    "    psnr = 20 * math.log10(max_value / math.sqrt(mse))\n",
    "    return psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HDhG7zyag8g6",
    "outputId": "7ed474c0-4712-4343-d545-df75059f4e62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR in terms of training:0.09003838477133778\n"
     ]
    }
   ],
   "source": [
    "#original psnr\n",
    "#psnr when calculated with the orignal and the ground truth\n",
    "psnr = 0\n",
    "for i in range(training_sample.shape[0]):\n",
    "    psnr += calculate_psnr(training_sample[i], testing_sample[i])\n",
    "    psnr /= training_sample.shape[0]\n",
    "print(f'PSNR in terms of training:{psnr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "yN7apCcgg8g6",
    "outputId": "8d730226-fe72-48e4-e28b-a51869f28aed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR in terms of predciton:0.023833554410302035\n"
     ]
    }
   ],
   "source": [
    "#predcition psnr\n",
    "#psnr when calculated with the prediction and the ground truth\n",
    "psnr = 0\n",
    "for i in range(training_sample.shape[0]):\n",
    "    psnr += calculate_psnr(predictions[i], testing_sample[i])\n",
    "    psnr /= training_sample.shape[0]\n",
    "print(f'PSNR in terms of predciton:{psnr}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
